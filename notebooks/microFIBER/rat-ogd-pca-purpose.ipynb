{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Everything about PCA in VAMPIRE Analysis\n",
    "\n",
    "27 March 2022\n",
    "\n",
    "Objective: Explore the purpose of PCA in VAMPIRE analysis.\n",
    "\n",
    "Hypothesis: PCA acts as a noise reduction tool and a dimensionality reduction tool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Baseline data setup\n",
    "\n",
    "### Baseline VAMPIRE analysis\n",
    "\n",
    "Baseline VAMPIRE analysis applies PCA on normalized contours, followed by K-Means clustering of principal components in the PC space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import vampire as vp\n",
    "import umap\n",
    "from sklearn import manifold\n",
    "vp.plot.set_plot_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_set_path = r'C:\\Files\\github-projects\\nance-lab-data\\microfiber\\ogd_severity_segmentations'\n",
    "output_path = r'C:\\Files\\github-projects\\nance-lab-data\\microfiber\\result\\result-2022-03-27'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract properties from all images for future use\n",
    "vp.extraction.extract_properties(img_set_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_info_df = pd.DataFrame({\n",
    "        'img_set_path': [img_set_path],\n",
    "        'output_path': [output_path],\n",
    "        'model_name': ['otsu'],\n",
    "        'num_points': [np.nan],\n",
    "        'num_clusters': [np.nan],\n",
    "        'num_pc': [np.nan],\n",
    "        'threshold': ['otsu'],\n",
    "    })\n",
    "vp.quickstart.build_models(build_info_df, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(output_path, 'model_otsu_(50_5_32)__otsu.pickle')\n",
    "vampire_model = vp.util.read_pickle(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_info_df = pd.DataFrame({\n",
    "        'img_set_path': [img_set_path],\n",
    "        'model_path': [model_path],\n",
    "        'output_path': [output_path],\n",
    "        'img_set_name': ['otsu'],\n",
    "    })\n",
    "vp.quickstart.apply_models(apply_info_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "property_path = os.path.join(output_path, 'apply-properties_otsu_on_otsu_(50_5_32)__.pickle')\n",
    "apply_properties_df = vp.util.read_pickle(property_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Data processing\n",
    "\n",
    "#### Label experimental metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_properties_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_df(properties_df, id_df, target_props, search_prop='filename'):\n",
    "    \"\"\"\n",
    "    properties_df : DataFrame\n",
    "        Contains \"filename\" column\n",
    "    id_df : DataFrame\n",
    "        Contains columns listed in `properties` and \"regex\" column.\n",
    "    target_props : list\n",
    "        List of property(s) to be added. Must match column name in `id_df`.\n",
    "    \"\"\"\n",
    "    properties_df = properties_df.copy()\n",
    "    properties_df[target_props] = np.nan\n",
    "    for i in range(id_df['regex'].size):\n",
    "        mask = properties_df[search_prop].astype(str).str.contains(id_df['regex'][i])\n",
    "        properties_df.loc[mask, target_props] = id_df.iloc[i, :][target_props].values\n",
    "    return properties_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_id_df = pd.read_excel(r'C:\\Files\\github-projects\\nance-lab-data\\microfiber\\slice-labels.xlsx')\n",
    "slice_id_df['regex'] = slice_id_df['slice_id'] + '_'\n",
    "slice_id_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_names = ['cortex', 'thalamus', 'hippocampus']\n",
    "region_id_df = pd.DataFrame({\n",
    "    'regex': region_names,\n",
    "    'region': region_names,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_properties_df = label_df(apply_properties_df, slice_id_df, ['slice_id', 'treatment'])\n",
    "apply_properties_df = label_df(apply_properties_df, region_id_df, ['region'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_properties_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "#### Label coloring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import to_hex\n",
    "color_id_df = pd.DataFrame({\n",
    "    'cluster_id': [0, 1, 2, 3, 4],\n",
    "    'color': [to_hex(plt.get_cmap('twilight')(np.linspace(0.1, 0.9, 5))[i]) for i in range(5)],\n",
    "})\n",
    "color_id_df['regex'] = color_id_df['cluster_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_colors = label_df(apply_properties_df, color_id_df, ['color'], 'cluster_id').color.values\n",
    "label_colors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Contour visual representations (This should be moved to KMeans clustering)\n",
    "\n",
    "The paper states that contours are represented by centroid reconstruction of each cluster. However, the mean contour of each cluster is used instead. Here, we comapre the two types of visualization.\n",
    "\n",
    "- Cluster centroid reconstruction (Paper)\n",
    "- Mean contour of each cluster (Implementation)\n",
    "\n",
    "### Cluster centroid reconstruction\n",
    "\n",
    "To reconstruct the shape, we use \n",
    "\n",
    "$$\\text{PCA reconstruction = (PC score)(principal directions) + mean}$$\n",
    "\n",
    "[Additional resource: How to reverse PCA and reconstruct original variables from several principal components?](https://stats.stackexchange.com/questions/229092/how-to-reverse-pca-and-reconstruct-original-variables-from-several-principal-comhttps://stats.stackexchange.com/questions/229092/how-to-reverse-pca-and-reconstruct-original-variables-from-several-principal-comhttps://stats.stackexchange.com/questions/229092/how-to-reverse-pca-and-reconstruct-original-variables-from-several-principal-comhttps://stats.stackexchange.com/questions/229092/how-to-reverse-pca-and-reconstruct-original-variables-from-several-principal-com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, vampire_model.num_clusters, figsize=(10, 2))\n",
    "principal_directions = vampire_model.principal_directions[:, :vampire_model.num_pc].T\n",
    "mean_contour = vampire_model.mean_aligned_contour\n",
    "for i in range(vampire_model.num_clusters):\n",
    "    # calculate reconstructions\n",
    "    centroid_pc = vampire_model.centroids[i]\n",
    "    centroid_coord = centroid_pc @ principal_directions + mean_contour\n",
    "    centroid_x = centroid_coord[:vampire_model.num_points]\n",
    "    centroid_y = centroid_coord[vampire_model.num_points:]\n",
    "    # plot reconstructions\n",
    "    axs[i].plot(centroid_x, centroid_y)\n",
    "    axs[i].axis('equal')\n",
    "    axs[i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Mean contour of each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "vp.plot.plot_contours(vampire_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Comparing the two methods, the cluster centroid reconstruction has less variation between groups than mean contour of each cluster. Small change from centroid reconstruction is a large change in the real contour space.\n",
    "\n",
    "We can plot some contours randomly selected representatives from each cluster as our standard for comparison. The mean contour of each cluster resembles the representatives the most, as expected, because overlay of the representatives are the mean representation of the contours.\n",
    "\n",
    "(I would call the centroid reconstruction as \"shape mode\" and the mean contour of each cluster as \"mean representation of shape mode\", but the original paper calls the mean contour as \"shape mode\".)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "vp.plot.plot_representatives(vampire_model, apply_properties_df, num_sample=50, random_state=1, alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "**Conclusion**: We will stick with mean contour of each cluster for visualization purposes. It's more easy to identify and associate features to them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## Baseline PCA analysis\n",
    "\n",
    "### PCA explained variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "vampire_model.num_pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(vampire_model.explained_variance_ratio, 'o-')\n",
    "plt.plot(vampire_model.num_pc, \n",
    "         vampire_model.explained_variance_ratio[vampire_model.num_pc], 'ro-')\n",
    "plt.xlabel('PC number')\n",
    "plt.ylabel('Explained variance ratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "^ The explained variance ratio of each PC decreases dramatically at first two. Elbow rule suggests that we can keep the first 2, 5, or 9 PCs, depending on our judgement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(vampire_model.cum_explained_variance_ratio, 'o-')\n",
    "plt.plot(vampire_model.num_pc, \n",
    "         vampire_model.cum_explained_variance_ratio[vampire_model.num_pc], 'ro-')\n",
    "plt.xlabel('Number of PC')\n",
    "plt.ylabel('Cumulative explained variance ratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "^ The cumulative explained variance ratio at 2, 5, and 9 is ~40%, 65%, and 75%. To cover 95% of total variance, the first 32 PCs are kept (shown in red).\n",
    "\n",
    "The plots suggests that PCA filters and denoises the data by neglecting non-contributing portion in the PC space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "### PC space visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = vp.analysis.pca_transform_contours(normalized_contours,\n",
    "                                       vampire_model.mean_aligned_contour,\n",
    "                                       vampire_model.principal_directions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Y[:, 0], Y[:, 1], s=1)\n",
    "plt.xlabel(f'PC 1 ({round(vampire_model.explained_variance_ratio[0]*100)}%)')\n",
    "plt.ylabel(f'PC 2 ({round(vampire_model.explained_variance_ratio[1]*100)}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "^ The first two PC captures ~42% total variance. There is no clear separation between clusters. Rather, we see a big cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Y[:, 0], Y[:, 1], s=1, color=label_colors)\n",
    "plt.xlabel(f'PC 1 ({round(vampire_model.explained_variance_ratio[0]*100)}%)')\n",
    "plt.ylabel(f'PC 2 ({round(vampire_model.explained_variance_ratio[1]*100)}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "^ After annotating with result of VAMPIRE analysis cluster, we see that the first two PC separates all clusters expect the dark cluster (cluster 3). \n",
    "\n",
    "```{hint}\n",
    "This may suggest that four clusters is enough to capture the shape variations. However, this may also be cautioned that only less than 50% of total variance is captured.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Y[:, 0], Y[:, 1], s=1, color=label_colors, alpha=0.1)\n",
    "plt.xlabel(f'PC 1 ({round(vampire_model.explained_variance_ratio[0]*100)}%)')\n",
    "plt.ylabel(f'PC 2 ({round(vampire_model.explained_variance_ratio[1]*100)}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "^ Changing alpha helps to gauge numbers of samples in each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Y[:, 0], Y[:, 2], s=1, color=label_colors, alpha=0.5)\n",
    "plt.xlabel(f'PC 1 ({round(vampire_model.explained_variance_ratio[0]*100)}%)')\n",
    "plt.ylabel(f'PC 3 ({round(vampire_model.explained_variance_ratio[2]*100)}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Y[:, 1], Y[:, 2], s=1, color=label_colors, alpha=0.5)\n",
    "plt.ylabel(f'PC 2 ({round(vampire_model.explained_variance_ratio[1]*100)}%)')\n",
    "plt.xlabel(f'PC 3 ({round(vampire_model.explained_variance_ratio[2]*100)}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "^ Plots of PC 2 and 3 also do not show effective separation of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "## Elimination of PCA for direct clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_contours = np.vstack(apply_properties_df['normalized_contour'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "direct_cluster_id_df, direct_centroids, inertia = vp.analysis.cluster_contours(normalized_contours, num_clusters=5, num_pc=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "direct_cluster_id_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "direct_df = pd.concat([apply_properties_df.drop(['cluster_id', 'distance_to_centroid'], axis=1), direct_cluster_id_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "direct_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "### Cluster centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, vampire_model.num_clusters, figsize=(10, 2))\n",
    "for i in range(vampire_model.num_clusters):\n",
    "    centroid_x = direct_centroids[i, :vampire_model.num_points]\n",
    "    centroid_y = direct_centroids[i, vampire_model.num_points:]\n",
    "    # plot reconstructions\n",
    "    axs[i].plot(centroid_x, centroid_y)\n",
    "    axs[i].axis('equal')\n",
    "    axs[i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "### Mean contour of each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, vampire_model.num_clusters, figsize=(10, 2))\n",
    "for i in range(vampire_model.num_clusters):\n",
    "    cluster_mask = direct_df['cluster_id'] == i\n",
    "    normalized_contour = np.vstack(direct_df[cluster_mask]['normalized_contour'])\n",
    "    mean_normalized_contour = normalized_contour.mean(axis=0)\n",
    "    contour_x = mean_normalized_contour[:vampire_model.num_points]\n",
    "    contour_y = mean_normalized_contour[vampire_model.num_points:]\n",
    "    axs[i].plot(contour_x, contour_y)\n",
    "    axs[i].axis('equal')\n",
    "    axs[i].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "vp.plot.plot_contours(vampire_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = vp.analysis.pca_transform_contours(normalized_contours,\n",
    "                                       vampire_model.mean_aligned_contour,\n",
    "                                       vampire_model.principal_directions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "## PCA on morphological parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "- https://stats.stackexchange.com/questions/183236/what-is-the-relation-between-k-means-clustering-and-pca\n",
    "- https://pair-code.github.io/understanding-umap/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
